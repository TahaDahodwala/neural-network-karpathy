# ğŸ§  Micrograd (Rebuilt from Scratch)

This repo is a pure Python implementation of **Andrej Karpathy's Micrograd** â€” a tiny scalar-valued autograd engine and neural network built from scratch using only fundamental Python.

> ğŸ“¹ Inspired by: [The spelled-out intro to neural networks and backpropagation](https://youtu.be/VMj-3S1tku0) by Andrej Karpathy

---

## ğŸ“š Project Overview

Over 5 Jupyter notebooks, this project carefully recreates the building blocks of neural networks and automatic differentiation, eventually training a mini MLP model from scratch.

All components are explained, implemented, and visualized step-by-step to aid deep understanding.

---

## ğŸ” File Breakdown

### âœ… `karpathy 001.ipynb` - Building the Engine
- Built the `Value` class from scratch.
- Implemented operator overloading: `+`, `*`, etc.
- Introduced the computational graph using Graphviz.
- Demonstrated a simple example of forward pass and manual gradient computation.

### âœ… `karpathy 002.ipynb` - Backprop from Scratch
- Implemented `.backward()` method for automatic differentiation.
- Explained backpropagation deeply using a simple computation graph.
- Walked through how gradients are calculated and flow backwards.

### âœ… `karpathy 003.ipynb` - Neuron by Neuron
- Defined a `Neuron` using the `Value` class.
- Implemented `tanh` activation function manually.
- Performed a manual forward + backward pass on a neuron.
- Encapsulated neuron logic for reuse.

### âœ… `karpathy 004.ipynb` - Math Extensions
- Added support for more operators: `-`, `/`, `**`, exponentiation.
- Reimplemented `tanh` using only basic math.
- Made the `Value` class behave like numbers for intuitive math chaining.

### âœ… `karpathy 005.ipynb` - Training a Neural Net
- Built:
  - `Neuron`
  - `Layer`
  - `MLP` (Multi-Layer Perceptron)
- Trained the MLP on a small dataset using manual gradient descent.
- All forward and backward passes handled via our own `Value` engine.
- Simulated PyTorch-style training loop using only NumPy + Python.

---

## ğŸ’¡ Key Concepts

| Concept               | Covered âœ… |
|-----------------------|-----------|
| Forward Propagation   | âœ…         |
| Backward Propagation  | âœ…         |
| Autograd Engine       | âœ…         |
| Activation Functions  | âœ… (tanh)  |
| Custom MLP Structure  | âœ…         |
| Manual Loss + Updates | âœ…         |

---

## ğŸš€ How to Run

1. Clone the repo:
```bash
git clone https://github.com/yourusername/micrograd-clone.git
cd micrograd-clone
```

2. Open any `.ipynb` file in Jupyter or VSCode.
3. Run the cells in order to see the magic happen âœ¨

---

## ğŸ™ Credits

- ğŸ“º [Andrej Karpathy](https://www.youtube.com/@karpathy)
- ğŸ”— [Original Micrograd Repo](https://github.com/karpathy/micrograd)
- ğŸ’» Rebuilt step-by-step by **Taha Dahodwala**

> ğŸ§  â€œUnderstanding comes from building.â€ â€” This repo reflects that.

---

## ğŸ“… Completed: June 25, 2025

